[model]
architecture = 'lstm'
input_dim = 50  # Will be determined dynamically
hidden_size = 128
num_layers = 2
dropout = 0.1
use_attention = true
use_residual = true
use_layer_norm = true
context_len = 168
pred_len = 24

[training]
batch_size = 16
lr = 0.0001
finetune_lr = 0.0001
max_epochs = 25
min_epochs = 0
patience = 5
weight_decay = 1e-5
max_grad_norm = 0.5
optimizer = 'adamw'
scheduler_type = 'cosine_restarts'
scheduler_T_0 = 10
scheduler_T_mult = 2
use_mixed_precision = true
gradient_accumulation_steps = 1
use_adaptive_grad_clip = false
adaptive_clip_value = 0.01

[features]
use_temporal_embeddings = true
use_context_stats = true
use_feature_interactions = true
include_lgbm_as_feature = false
temporal_embedding_dim = 32

[loss]
lambda_rc = 0.1
lambda_comfort = 0.01
lambda_smooth = 0.001
use_rc_loss = true
use_comfort_loss = true
use_smooth_loss = true
use_adaptive_weights = false
use_focal_loss = false
focal_alpha = 1.0
focal_gamma = 2.0
use_quantile_loss = false
data_loss_type = 'mse'

